{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lyeq7xuVpqh",
        "outputId": "788494f6-c352-4456-dacb-746548d3c096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L08qwPoxENZ"
      },
      "source": [
        "#Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4VsNz-sbxk"
      },
      "source": [
        "##High - High Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set is saved in csv file which contains 400 dataset that has sentiments from customers about the ice cream taste in a ice cream store, the datase is collected with the help of kaggle and have an original size of almost 5000+ dataset."
      ],
      "metadata": {
        "id": "lQstrJFHQpqu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "lxkR_wXFvsd1",
        "outputId": "2d53149f-9d57-478d-a311-5de41993bc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Losses: {'textcat': 73.2360592186451}\n",
            "Epoch 2/10 Losses: {'textcat': 66.51440756539341}\n",
            "Epoch 3/10 Losses: {'textcat': 59.37238605113987}\n",
            "Epoch 4/10 Losses: {'textcat': 47.755903024564425}\n",
            "Epoch 5/10 Losses: {'textcat': 41.60347930388314}\n",
            "Epoch 6/10 Losses: {'textcat': 34.069622301405616}\n",
            "Epoch 7/10 Losses: {'textcat': 20.107663974529416}\n",
            "Epoch 8/10 Losses: {'textcat': 15.733907981540499}\n",
            "Epoch 9/10 Losses: {'textcat': 9.756341760908066}\n",
            "Epoch 10/10 Losses: {'textcat': 8.754340456397685}\n",
            "Performance Evaluation Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Not Satisfied       0.89      0.63      0.74        62\n",
            "    Satisfied       0.70      0.91      0.79        58\n",
            "\n",
            "     accuracy                           0.77       120\n",
            "    macro avg       0.79      0.77      0.76       120\n",
            " weighted avg       0.80      0.77      0.76       120\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhile True:\\n    user_input = input(\"Enter a text for sentiment analysis (or type \\'exit\\' to quit): \")\\n    if user_input.lower() == \\'exit\\':\\n        break\\n    prediction = predict_sentiment(user_input)\\n    print(\"Sentiment Prediction:\")\\n    print(prediction)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "# Load a blank model and add text classifier\n",
        "# Path to the CSV file in Google Drive\n",
        "csv_file_path5 = '/content/drive/MyDrive/4th Year/ELECTIVE 4/Exercise 4.1/Sentiment Analysis/High_size_HIGH_quality_Sentiment.csv'\n",
        "\n",
        "# Load the CSV file using pandas\n",
        "df = pd.read_csv(csv_file_path5)\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Convert the DataFrame to the format required by spaCy\n",
        "data = []\n",
        "for index, row in df.iterrows():\n",
        "    text = remove_stopwords(row['text'])  # Remove stopwords\n",
        "    label = {'Satisfied': row['Final_Rating'] == 'Satisfied', 'Not Satisfied': row['Final_Rating'] == 'Not Satisfied'}\n",
        "    data.append((text, {'cats': label}))\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_data5, test_data5 = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Load a blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the text classification pipeline\n",
        "textcat = nlp.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for the text classification\n",
        "textcat.add_label(\"Satisfied\")\n",
        "textcat.add_label(\"Not Satisfied\")\n",
        "\n",
        "# Training the model\n",
        "def train_model(data, n_iter=10):\n",
        "    random.shuffle(data)\n",
        "    optimizer = nlp.begin_training()\n",
        "    for epoch in range(n_iter):\n",
        "        losses = {}\n",
        "        for text, annotations in data:\n",
        "            doc = nlp.make_doc(text)\n",
        "            example = Example.from_dict(doc, annotations)\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "        print(f\"Epoch {epoch + 1}/{n_iter} Losses: {losses}\")\n",
        "\n",
        "train_model(train_data5)\n",
        "\n",
        "# Function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    text = remove_stopwords(text)  # Remove stopwords from user input\n",
        "    doc = nlp(text)\n",
        "    return doc.cats\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for text, annotations in test_data5:\n",
        "    doc = nlp(text)\n",
        "    y_true.append(max(annotations[\"cats\"], key=annotations[\"cats\"].get))\n",
        "    y_pred.append(max(doc.cats, key=doc.cats.get))\n",
        "\n",
        "# Print evaluation results\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=[\"Not Satisfied\", \"Satisfied\"])\n",
        "print(\"Performance Evaluation Report:\\n\", report)\n",
        "\n",
        "# User input and sentiment analysis\n",
        "\"\"\"\n",
        "while True:\n",
        "    user_input = input(\"Enter a text for sentiment analysis (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    prediction = predict_sentiment(user_input)\n",
        "    print(\"Sentiment Prediction:\")\n",
        "    print(prediction)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging\n"
      ],
      "metadata": {
        "id": "ExSA-zZLUGx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used has 50 data which contains text and it was predfined according to its part of speeach. Each text in are defined whether it is a 'noun', 'verb', 'adp', etc."
      ],
      "metadata": {
        "id": "ouBA6TuRwXpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##High - High Quality"
      ],
      "metadata": {
        "id": "QrB_m6-5fFna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Load pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample data with individual words and their corresponding labels\n",
        "data5 = [(\"Artificial Intelligence is transforming industries across the globe.\",[\"ADJ\", \"NOUN\", \"AUX\", \"VERB\", \"NOUN\", \"ADP\", \"DET\", \"NOUN\"]),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\",[\"DET\", \"ADJ\", \"ADJ\", \"NOUN\", \"VERB\", \"ADP\", \"DET\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Data scientists analyze vast amounts of information every day.\",[\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\", \"ADP\", \"NOUN\", \"DET\", \"NOUN\"]),\n",
        "    (\"Climate change is a pressing global issue that requires immediate action.\",[\"NOUN\", \"NOUN\", \"AUX\", \"DET\", \"ADJ\", \"ADJ\", \"NOUN\", \"PRON\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Renewable energy sources are crucial for a sustainable future.\",[\"ADJ\", \"NOUN\", \"NOUN\", \"AUX\", \"ADJ\", \"ADP\", \"DET\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Machine learning techniques improve decision making in various fields.\",[\"NOUN\", \"NOUN\", \"NOUN\", \"VERB\", \"NOUN\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Natural language processing enables computers to understand human language.\",[\"ADJ\", \"NOUN\", \"NOUN\", \"VERB\", \"NOUN\", \"ADP\", \"NOUN\", \"NOUN\"]),\n",
        "    (\"The stock market is volatile and requires careful analysis.\",[\"DET\", \"NOUN\", \"AUX\", \"ADJ\", \"CONJ\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Artificial Intelligence can enhance customer experience.\",[\"ADJ\", \"NOUN\", \"AUX\", \"VERB\", \"NOUN\", \"NOUN\"]),\n",
        "    (\"Robotics is the future of automation.\",[\"NOUN\", \"AUX\", \"DET\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Blockchain technology revolutionizes digital transactions.\", [\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Healthcare innovations improve patient outcomes significantly.\", [\"NOUN\", \"NOUN\", \"VERB\", \"NOUN\", \"ADJ\"]),\n",
        "    (\"Social media platforms connect people around the world.\", [\"ADJ\", \"NOUN\", \"VERB\", \"NOUN\", \"ADP\", \"DET\", \"NOUN\"]),\n",
        "    (\"Cybersecurity measures protect sensitive information effectively.\", [\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\", \"ADJ\"]),\n",
        "    (\"Augmented reality enhances user experiences in gaming.\", [\"ADJ\", \"NOUN\", \"VERB\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Sustainable practices reduce environmental impact dramatically.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\", \"ADJ\"]),\n",
        "    (\"E-commerce continues to grow rapidly across industries.\", [\"NOUN\", \"VERB\", \"PART\", \"ADJ\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Cloud computing enables flexible data storage solutions.\", [\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\", \"NOUN\"]),\n",
        "    (\"5G technology facilitates faster internet connections.\", [\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Remote work offers new opportunities for collaboration.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Artificial Intelligence will reshape the future of work.\", [\"ADJ\", \"NOUN\", \"AUX\", \"VERB\", \"DET\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"The rise of renewable energy is crucial for sustainability.\", [\"DET\", \"NOUN\", \"ADP\", \"ADJ\", \"NOUN\", \"AUX\", \"ADJ\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Advancements in technology drive innovation and growth.\", [\"NOUN\", \"ADP\", \"NOUN\", \"VERB\", \"NOUN\", \"CONJ\", \"NOUN\"]),\n",
        "    (\"Smart cities utilize technology to improve quality of life.\", [\"ADJ\", \"NOUN\", \"VERB\", \"NOUN\", \"ADP\", \"VERB\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Virtual reality provides immersive experiences.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"The healthcare industry is evolving rapidly.\", [\"DET\", \"NOUN\", \"NOUN\", \"AUX\", \"VERB\", \"ADJ\"]),\n",
        "    (\"Internet of Things connects everyday devices.\", [\"NOUN\", \"ADP\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Data privacy is becoming a significant concern.\", [\"NOUN\", \"NOUN\", \"AUX\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Telemedicine is transforming patient care.\", [\"NOUN\", \"AUX\", \"VERB\", \"NOUN\", \"NOUN\"]),\n",
        "    (\"Blockchain offers a secure method for transactions.\", [\"NOUN\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Climate action is essential for future generations.\", [\"NOUN\", \"NOUN\", \"AUX\", \"ADJ\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Social entrepreneurship creates positive change.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Digital marketing strategies engage customers effectively.\", [\"ADJ\", \"NOUN\", \"NOUN\", \"VERB\", \"NOUN\", \"ADJ\"]),\n",
        "    (\"Artificial intelligence enhances decision-making processes.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Big data analytics reveals insights about consumer behavior.\", [\"ADJ\", \"NOUN\", \"NOUN\", \"VERB\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Sustainable agriculture practices benefit the environment.\", [\"ADJ\", \"NOUN\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"]),\n",
        "    (\"The gig economy provides flexible work opportunities.\", [\"DET\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\", \"NOUN\"]),\n",
        "    (\"Smartphones have changed the way we communicate.\", [\"NOUN\", \"AUX\", \"VERB\", \"DET\", \"NOUN\", \"PRON\", \"VERB\"]),\n",
        "    (\"Renewable resources are vital for energy security.\", [\"ADJ\", \"NOUN\", \"AUX\", \"ADJ\", \"ADP\", \"NOUN\"]),\n",
        "    (\"Urbanization leads to increased infrastructure demands.\", [\"NOUN\", \"VERB\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Crowdsourcing taps into collective intelligence.\", [\"NOUN\", \"VERB\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"The sharing economy encourages collaborative consumption.\", [\"DET\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Smart home technology automates daily tasks.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"E-learning platforms are revolutionizing education.\", [\"NOUN\", \"NOUN\", \"AUX\", \"VERB\", \"NOUN\"]),\n",
        "    (\"3D printing enables rapid prototyping.\", [\"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Artificial intelligence is improving healthcare diagnostics.\", [\"ADJ\", \"NOUN\", \"AUX\", \"VERB\", \"NOUN\", \"NOUN\"]),\n",
        "    (\"Fintech solutions streamline financial processes.\", [\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"Workplace diversity enhances creativity and innovation.\", [\"NOUN\", \"NOUN\", \"VERB\", \"ADJ\", \"CONJ\", \"NOUN\"]),\n",
        "    (\"Online communities foster collaboration and support.\", [\"ADJ\", \"NOUN\", \"VERB\", \"NOUN\", \"CONJ\", \"NOUN\"]),\n",
        "    (\"Artificial intelligence applications are expanding rapidly.\", [\"ADJ\", \"NOUN\", \"NOUN\", \"AUX\", \"VERB\", \"ADJ\"]),\n",
        "    (\"Remote learning offers new educational opportunities.\", [\"ADJ\", \"NOUN\", \"VERB\", \"ADJ\", \"ADJ\", \"NOUN\"]),\n",
        "    (\"The future of work is hybrid and flexible.\", [\"DET\", \"NOUN\", \"ADP\", \"NOUN\", \"AUX\", \"ADJ\"])\n",
        "]\n",
        "\n",
        "import re\n",
        "\n",
        "all_true_pos_tags5 = []\n",
        "all_predicted_pos_tags5 = []\n",
        "\n",
        "# Iterate through training data\n",
        "for text, true_pos_tags5 in data5:\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Process the cleaned text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract predicted POS tags and true tags, excluding stopwords\n",
        "    predicted_pos_tags5 = [token.pos_ for token in doc if not token.is_stop]\n",
        "    true_pos_tags_filtered = [true_tag for token, true_tag in zip(doc, true_pos_tags5) if not token.is_stop]\n",
        "\n",
        "    # Extend lists with true and predicted tags for evaluation\n",
        "    all_true_pos_tags5.extend(true_pos_tags_filtered)\n",
        "    all_predicted_pos_tags5.extend(predicted_pos_tags5)\n",
        "\n",
        "# Ensure both lists are the same length to avoid ValueError\n",
        "if len(all_true_pos_tags5) != len(all_predicted_pos_tags5):\n",
        "    min_length = min(len(all_true_pos_tags5), len(all_predicted_pos_tags5))\n",
        "    all_true_pos_tags5 = all_true_pos_tags5[:min_length]\n",
        "    all_predicted_pos_tags5 = all_predicted_pos_tags5[:min_length]\n",
        "\n",
        "# Evaluate\n",
        "accuracy5 = accuracy_score(all_true_pos_tags5, all_predicted_pos_tags5)\n",
        "precision5 = precision_score(all_true_pos_tags5, all_predicted_pos_tags5, average='weighted', zero_division=0)\n",
        "recall5 = recall_score(all_true_pos_tags5, all_predicted_pos_tags5, average='weighted', zero_division=0)\n",
        "f15 = f1_score(all_true_pos_tags5, all_predicted_pos_tags5, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Accuracy:\", accuracy5)\n",
        "print(\"Precision:\", precision5)\n",
        "print(\"Recall:\", recall5)\n",
        "print(\"F1-Score:\", f15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqFJnRjOfFPq",
        "outputId": "8c2a8f3c-c01d-48a2-a2dd-f68d3c6b2805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4318181818181818\n",
            "Precision: 0.44836075939362324\n",
            "Recall: 0.4318181818181818\n",
            "F1-Score: 0.4390476389929265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Categorizer"
      ],
      "metadata": {
        "id": "ydx9PsAwjONs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set used is manually inputted to the 'train data' and 'test data' object and consists of a total of 100 data. These data are phrases and categorized either it is HAM or SPAM messages."
      ],
      "metadata": {
        "id": "2uCKmlSav-0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High - High Quality"
      ],
      "metadata": {
        "id": "NCEAvLYdrYLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC5 = spacy.blank(\"en\")\n",
        "textcat5 = nlpTC5.add_pipe(\"textcat\", last=True)\n",
        "\n",
        "# Add labels for classification\n",
        "textcat5.add_label(\"SPAM\")\n",
        "textcat5.add_label(\"HAM\")\n",
        "\n",
        "# Function to lowercase and remove special characters\n",
        "def process_text(text):\n",
        "    return re.sub(r'[^a-z0-9\\s]', '', text.lower())\n",
        "\n",
        "# Example training data (90 samples)\n",
        "train_data5 = [\n",
        "    (\"youve won a 1000 gift card\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"hi how have you been\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"claim your free prize today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your order has shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"congratulations you are selected for a special offer\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets meet for lunch soon\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"urgent update your account information now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"looking forward to our meeting\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have won a free vacation\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"just checking in hope all is well\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"get paid for your opinion\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"dont forget about our appointment\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you are a lucky winner\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"meeting is confirmed for tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"win a new car enter now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"can we reschedule our meeting\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have a package waiting\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your subscription is about to expire\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"congratulations youve been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"hope to see you at the event\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"earn money from home\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets catch up soon\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have a new message\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"reminder your appointment is tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"claim your free gift now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"looking forward to our lunch\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youre a winner\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"dont miss this opportunity\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"thank you for your purchase\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have won a free trial\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"it was great seeing you\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"exclusive deal just for you\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"how can i assist you today\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve been selected for a free vacation\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your feedback is important to us\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"hurry offer expires soon\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets discuss your project\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve won a 500 shopping spree\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"hows everything going\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"get a free trial of our service\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"were excited to have you\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"last chance to claim your prize\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"thanks for being a valued customer\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve been selected for a special offer\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"dont forget to register\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"congratulations you have won a gift card\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"get a new laptop for free\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"hope to hear from you soon\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve won a luxury cruise\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets plan a call\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have an important message\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"meeting has been postponed\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"claim your bonus now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your application has been approved\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"congratulations on your achievement\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets schedule a followup\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you are eligible for a discount\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your subscription has been renewed\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"win a free iphone\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"it was great catching up\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve been chosen for a special reward\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"we value your opinion\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"last chance to win\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"dont miss this opportunity to save\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your feedback is appreciated\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have a meeting scheduled\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve won a new tv\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"looking forward to our collaboration\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"claim your free bonus today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your document is ready\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"you have an exclusive invitation\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"hope youre having a great day\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve been selected for an amazing offer\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets connect soon\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve won a free membership\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"your support is important to us\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "]\n",
        "\n",
        "\n",
        "# Example test data (10 samples)\n",
        "test_data5 =  [\n",
        "    (\"youve won a free cruise!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"just wanted to check in on you\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"congratulations! you have a new reward\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"lets schedule a meeting next week\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"youve been selected for a special prize\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"thank you for your feedback\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"win a free smartphone now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"hope you are doing well\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"urgent: claim your prize today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"reminder: your appointment is scheduled\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}})\n",
        "]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Clean training data by removing stopwords\n",
        "train_data5 = [\n",
        "    (remove_stopwords(text), annotations) for text, annotations in train_data5\n",
        "]\n",
        "\n",
        "# Training the model\n",
        "optimizer5 = nlpTC5.begin_training()\n",
        "for text5, annotations5 in train_data5:\n",
        "    doc5 = nlpTC5.make_doc(text5)\n",
        "    example5 = Example.from_dict(doc5, annotations5)\n",
        "    nlpTC5.update([example5], drop=0.5, sgd=optimizer5)\n",
        "\n",
        "# Evaluating the model\n",
        "y_true5 = []\n",
        "y_pred5 = []\n",
        "\n",
        "for text5, annotations5 in test_data5:\n",
        "    doc5 = nlpTC5(text5)\n",
        "    y_true5.append(max(annotations5[\"cats\"], key=annotations5[\"cats\"].get))\n",
        "    y_pred5.append(max(doc5.cats, key=doc5.cats.get))\n",
        "\n",
        "# Generate classification report\n",
        "report5 = classification_report(y_true5, y_pred5, target_names=[\"HAM\", \"SPAM\"])\n",
        "print(\"Performance Evaluation Report:\\n\", report5)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email5(email5):\n",
        "    processed_email5 = process_text(email5)\n",
        "    doc5 = nlpTC5(processed_email5)\n",
        "    spam_score5 = doc5.cats['SPAM']\n",
        "    ham_score5 = doc5.cats['HAM']\n",
        "    return \"SPAM\" if spam_score5 > ham_score5 else \"HAM\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3El-XwQorbo8",
        "outputId": "1e32b011-19e1-4b21-84cd-2768f905520a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Evaluation Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         HAM       1.00      0.40      0.57         5\n",
            "        SPAM       0.62      1.00      0.77         5\n",
            "\n",
            "    accuracy                           0.70        10\n",
            "   macro avg       0.81      0.70      0.67        10\n",
            "weighted avg       0.81      0.70      0.67        10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\", last=True)\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")\n",
        "\n",
        "# Expanded example training data\n",
        "train_data5 = [\n",
        "    (\"You've won a $1000 gift card!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hi, how have you been?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your free prize today!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your order has shipped.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You are selected for a special offer.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Let's meet for lunch soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Urgent: Update your account information now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Looking forward to our meeting.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have won a free vacation!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Just checking in. Hope all is well!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Get paid for your opinion!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Don't forget about our appointment.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You are a lucky winner!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting is confirmed for tomorrow.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a new car! Enter now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our meeting?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have a package waiting!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your subscription is about to expire.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hope to see you at the event.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Earn money from home!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Let's catch up soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have a new message!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Reminder: Your appointment is tomorrow.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your free gift now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Looking forward to our lunch.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You're a winner!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Don't miss this opportunity!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Thank you for your purchase!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have won a free trial!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"It was great seeing you!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive deal just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"How can I assist you today?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've been selected for a free vacation!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your feedback is important to us.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Hurry! Offer expires soon!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Let's discuss your project.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've won a $500 shopping spree!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"How's everything going?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Get a free trial of our service!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"We're excited to have you!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Last chance to claim your prize!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Thanks for being a valued customer.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've been selected for a special offer!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Don't forget to register!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You have won a gift card!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your invoice is attached.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Get a new laptop for free!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hope to hear from you soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've won a luxury cruise!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Let's plan a call.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have an important message!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting has been postponed.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your bonus now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your application has been approved.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations on your achievement!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Let's schedule a follow-up.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You are eligible for a discount!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your subscription has been renewed.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a free iPhone!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"It was great catching up!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've been chosen for a special reward!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"We value your opinion.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Last chance to win!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Don't miss this opportunity to save!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your feedback is appreciated.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have a meeting scheduled.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've won a new TV!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Looking forward to our collaboration.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your free bonus today!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your document is ready.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You have an exclusive invitation!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hope you're having a great day!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've been selected for an amazing offer!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Let's connect soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've won a free membership!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your support is important to us.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've won a new smartphone!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Just a reminder about our meeting tomorrow.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You have a new prize waiting.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hope you are doing well!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your free vacation!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your feedback is valuable.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a gift card today!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Looking forward to your response.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You've been selected for a special event.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Thank you for your cooperation.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}})\n",
        "]\n",
        "# Split the data into training and test sets (70-30 split)\n",
        "train_data5, test_data5 = train_test_split(train_data5, test_size=0.3, random_state=42)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Training the model\n",
        "optimizer = nlpTC.begin_training()\n",
        "for text5, annotations in train_data5:  # Use train_data5 here\n",
        "    text5 = text5.lower()\n",
        "    text5 = remove_stopwords(text5)\n",
        "    doc = nlpTC.make_doc(text5)\n",
        "\n",
        "    example5 = Example.from_dict(doc, annotations)\n",
        "    nlpTC.update([example5], drop=0.5, sgd=optimizer)  # Use example5 here\n",
        "\n",
        "# Evaluating the model\n",
        "y_true5 = []\n",
        "y_pred5 = []\n",
        "\n",
        "for text5, annotations in test_data5:  # Use test_data5 here\n",
        "    doc = nlpTC(text5)\n",
        "    y_true5.append(max(annotations[\"cats\"], key=annotations[\"cats\"].get))\n",
        "    y_pred5.append(max(doc.cats, key=doc.cats.get))\n",
        "\n",
        "# Generate classification report\n",
        "report5 = classification_report(y_true5, y_pred5, target_names=[\"HAM\", \"SPAM\"])  # Use y_true5 and y_pred5\n",
        "print(\"Performance Evaluation Report:\\n\", report5)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "    doc = nlpTC(email)\n",
        "    spam_score5 = doc.cats['SPAM']\n",
        "    ham_score5 = doc.cats['HAM']\n",
        "    return \"SPAM\" if spam_score5 > ham_score5 else \"HAM\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rpOSWDW9dON",
        "outputId": "fca9e995-38d3-4306-9f28-4ad53606966a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Evaluation Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         HAM       1.00      0.75      0.86        12\n",
            "        SPAM       0.83      1.00      0.91        15\n",
            "\n",
            "    accuracy                           0.89        27\n",
            "   macro avg       0.92      0.88      0.88        27\n",
            "weighted avg       0.91      0.89      0.89        27\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named Entity Recognition"
      ],
      "metadata": {
        "id": "k24XJ2chtNSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medium sized dataset in this model is composed of 50 data which also contains text but different to the POS, only named entity are recognized and labeld depending on its role to the given sentence."
      ],
      "metadata": {
        "id": "60_PpGipRe7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Medium -Low Quality"
      ],
      "metadata": {
        "id": "emLgVzGpLy1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample training data (text and true entity annotations)\n",
        "training_data3= [\n",
        "    (\"Microsoft acquired GitHub for $7.5 billion.\", [(0, 9, \"ORG\"), (18, 24, \"ORG\")]),\n",
        "    (\"The Eiffel Tower is located in Paris.\", [(4, 15, \"LOC\"), (27, 32, \"GPE\")]),\n",
        "    (\"Tesla's CEO, Elon Musk, announced new plans.\", [(0, 5, \"ORG\"), (13, 22, \"PERSON\")]),\n",
        "    (\"Amazon is expanding its services in Germany.\", [(0, 6, \"ORG\"), (30, 37, \"GPE\")]),\n",
        "    (\"The Great Wall of China attracts millions of tourists.\", [(4, 18, \"LOC\"), (22, 29, \"GPE\")]),\n",
        "    (\"Netflix released its latest series in July 2023.\", [(0, 6, \"ORG\"), (28, 32, \"DATE\")]),\n",
        "    (\"Barack Obama served as the 44th President of the United States.\", [(0, 12, \"PERSON\"), (41, 56, \"GPE\")]),\n",
        "    (\"The Amazon rainforest is vital for our planet.\", [(4, 11, \"LOC\")]),\n",
        "    (\"Apple's new iPhone will be available in October.\", [(0, 5, \"ORG\"), (30, 37, \"DATE\")]),\n",
        "    (\"Walt Disney World attracts millions of visitors each year.\", [(0, 4, \"ORG\"), (26, 32, \"LOC\")]),\n",
        "    (\"Facebook is launching a new feature next month.\", [(0, 8, \"ORG\"), (34, 40, \"DATE\")]),\n",
        "    (\"The Louvre Museum is one of the largest art museums in the world.\", [(4, 20, \"ORG\")]),\n",
        "    (\"Bill Gates founded Microsoft in 1975.\", [(0, 10, \"PERSON\"), (16, 24, \"ORG\"), (28, 32, \"DATE\")]),\n",
        "    (\"The Amazon River flows through Brazil and Peru.\", [(4, 10, \"LOC\"), (24, 30, \"GPE\"), (35, 39, \"GPE\")]),\n",
        "    (\"Samsung Electronics reported record profits this quarter.\", [(0, 18, \"ORG\")]),\n",
        "    (\"The United Nations was established after World War II.\", [(4, 20, \"ORG\"), (38, 42, \"EVENT\")]),\n",
        "    (\"Beyonc√© performed at the Super Bowl in 2016.\", [(0, 7, \"PERSON\"), (20, 31, \"EVENT\"), (35, 39, \"DATE\")]),\n",
        "    (\"The Great Barrier Reef is a UNESCO World Heritage site.\", [(4, 22, \"LOC\")]),\n",
        "    (\"China is hosting the next Olympic Games.\", [(0, 5, \"GPE\"), (27, 34, \"EVENT\")]),\n",
        "    (\"Stephen Hawking published a new book in 2018.\", [(0, 15, \"PERSON\"), (29, 33, \"DATE\")]),\n",
        "    (\"Twitter is experiencing an increase in user engagement.\", [(0, 6, \"ORG\")]),\n",
        "    (\"NASA is planning a mission to Mars.\", [(0, 4, \"ORG\"), (25, 29, \"LOC\")]),\n",
        "    (\"The Tesla Model S is gaining popularity among electric vehicles.\", [(4, 15, \"ORG\")]),\n",
        "    (\"London is the capital city of the United Kingdom.\", [(0, 6, \"GPE\"), (30, 48, \"GPE\")]),\n",
        "    (\"Harry Potter is a fictional character created by J.K. Rowling.\", [(0, 12, \"PERSON\"), (44, 48, \"PERSON\")]),\n",
        "    (\"The Eiffel Tower attracts millions of tourists each year.\", [(4, 15, \"LOC\")]),\n",
        "    (\"Elon Musk is the CEO of SpaceX.\", [(0, 9, \"PERSON\"), (24, 30, \"ORG\")]),\n",
        "    (\"The Pacific Ocean is the largest ocean on Earth.\", [(4, 17, \"LOC\")]),\n",
        "    (\"Sony unveiled a new gaming console this year.\", [(0, 4, \"ORG\"), (29, 33, \"DATE\")]),\n",
        "    (\"Barack Obama delivered a speech at the Democratic National Convention.\", [(0, 12, \"PERSON\"), (37, 47, \"EVENT\")]),\n",
        "    (\"Google launched a new AI tool last week.\", [(0, 6, \"ORG\"), (30, 34, \"DATE\")]),\n",
        "    (\"The Statue of Liberty is in New York City.\", [(4, 21, \"LOC\"), (25, 38, \"GPE\")]),\n",
        "    (\"Oprah Winfrey will host a new talk show.\", [(0, 12, \"PERSON\")]),\n",
        "    (\"The Great Pyramid of Giza is a marvel of engineering.\", [(4, 30, \"LOC\")]),\n",
        "    (\"Elon Musk plans to send humans to Mars.\", [(0, 9, \"PERSON\"), (29, 33, \"LOC\")]),\n",
        "    (\"The Academy Awards are held annually.\", [(4, 20, \"EVENT\")]),\n",
        "    (\"Nokia announced a partnership with Microsoft.\", [(0, 5, \"ORG\"), (36, 44, \"ORG\")]),\n",
        "    (\"Queen Elizabeth II reigned for decades.\", [(0, 15, \"PERSON\")]),\n",
        "    (\"The Amazon is the largest rainforest in the world.\", [(4, 11, \"LOC\")]),\n",
        "    (\"Facebook acquired Instagram in 2012.\", [(0, 8, \"ORG\"), (21, 29, \"ORG\"), (33, 37, \"DATE\")])\n",
        "]\n",
        "\n",
        "# Initialize lists for storing true and predicted entities\n",
        "all_true_entities3 = []\n",
        "all_pred_entities3 = []\n",
        "\n",
        "# Iterate through training data\n",
        "for text, true_entities in training_data3:\n",
        "    # Run NER model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Predicted entities from the model\n",
        "    pred_entities3 = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Store true and predicted entities for all examples\n",
        "    all_true_entities3.extend([(ent[0], ent[1], ent[2]) for ent in true_entities])\n",
        "    all_pred_entities3.extend(pred_entities3)\n",
        "\n",
        "# Convert to binary classification (1 for correct entity, 0 for incorrect)\n",
        "y_true3 = [1 if ent in all_true_entities3 else 0 for ent in all_pred_entities3]\n",
        "y_pred3 = [1 for _ in all_pred_entities3]  # Assuming all predictions are correct for now\n",
        "\n",
        "# Calculate Precision, Recall, F1, and Accuracy\n",
        "precision3 = precision_score(y_true3, y_pred3)\n",
        "recall3 = recall_score(y_true3, y_pred3)\n",
        "f13 = f1_score(y_true3, y_pred3)\n",
        "accuracy3 = accuracy_score(y_true3, y_pred3)\n",
        "\n",
        "print(\"Accuracy:\", accuracy3)\n",
        "print(\"Precision:\", precision3)\n",
        "print(\"Recall:\", recall3)\n",
        "print(\"F1-Score:\", f13)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmbka3B_L1Cn",
        "outputId": "bdd2c7b5-56e9-4c28-b7e3-6c74a87ae317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.28205128205128205\n",
            "Precision: 0.28205128205128205\n",
            "Recall: 1.0\n",
            "F1-Score: 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarizer"
      ],
      "metadata": {
        "id": "EAFUjgLHQGit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model (small for demonstration purposes)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "ge4zrdEMQaDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1012cd3d-d962-4067-b9b3-57f431943f73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used in this model is an article that was gathered and collected from GMA NEWS online, the text article is then fed to the previously created summarizer in which becomes the reference summary that will be used to identify the performance of the summarizer."
      ],
      "metadata": {
        "id": "4yeRkHr-R5kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Small - Low Quality"
      ],
      "metadata": {
        "id": "Xme03wVlQI7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original text to summarize\n",
        "text = \"\"\"CEBU CITY, Philippines ‚Äì Former Cebu City Mayor Tomas Osme√±a vowed to reveal more information about the extra-judicial killings (EJKs) here in the next House committee hearing. Osme√±a made this statement on Thursday, September 26, after announcing that the hearing scheduled this Friday, September 27, has been postponed to mid-October. However, the former mayor, who was once again invited to shed more light about reports of EJKs here, shared that this development would allow him to present and reveal more information, including records from victims of EJKs. ‚ÄúI‚Äôm going to use the extra time to incorporate additional information into my presentation. I already have plenty of information, both from my own records and from victims who have approached me in private,‚Äù he wrote. ‚ÄúPlenty of people have come forward. If anyone else wants to talk to me privately, as always I can be contacted directly at 0917 329 9999,‚Äù added Osme√±a. Osme√±a served as a resource speaker in the ongoing House quad committee hearing on EJKs and Philippine Offshore Gambling Operators (Pogos).\"\"\"\n",
        "\n",
        "# Reference summary with exact sentences from the original text\n",
        "reference_summary = \"\"\"CEBU CITY, Philippines ‚Äì Former Cebu City Mayor Tomas Osme√±a  to reveal more information about the extra-judicial killings (EJKs) here in the next House committee hearing. However, the former mayor, who was once again invited to shed more light about reports of EJKs here, shared that this development would allow him to present and reveal more information, including records from victims of EJKs. Osme√±a served as a resource speaker in the ongoing House quad committee hearing on EJKs and Philippine Offshore Gambling Operators (Pogos).\n",
        "\"\"\"\n",
        "\n",
        "# Extractive summarization function\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    return sentences[:num_sentences]  # Select the first 'num_sentences' sentences\n",
        "\n",
        "# Generate summary using the extractive method\n",
        "generated_summary = extractive_summary(text)\n",
        "generated_summary_text = ' '.join(generated_summary)\n",
        "\n",
        "# Tokenize the reference summary into sentences\n",
        "reference_sentences = sent_tokenize(reference_summary)\n",
        "\n",
        "# Tokenize the original text into sentences\n",
        "original_sentences = sent_tokenize(text)\n",
        "\n",
        "# Create binary relevance labels for each sentence in the original text\n",
        "# y_true: 1 if the sentence is in the reference summary, else 0\n",
        "# y_pred: 1 if the sentence is in the generated summary, else 0\n",
        "y_true = [1 if sent in reference_sentences else 0 for sent in original_sentences]\n",
        "y_pred = [1 if sent in generated_summary else 0 for sent in original_sentences]\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score, and Accuracy\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Generated Summary: {generated_summary_text}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(generated_summary_text)"
      ],
      "metadata": {
        "id": "-bj34lF_QFYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14c3f03-1ad6-4c3e-e847-59dd16f39443"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: CEBU CITY, Philippines ‚Äì Former Cebu City Mayor Tomas Osme√±a vowed to reveal more information about the extra-judicial killings (EJKs) here in the next House committee hearing. Osme√±a made this statement on Thursday, September 26, after announcing that the hearing scheduled this Friday, September 27, has been postponed to mid-October. However, the former mayor, who was once again invited to shed more light about reports of EJKs here, shared that this development would allow him to present and reveal more information, including records from victims of EJKs.\n",
            "Accuracy: 0.62\n",
            "Precision: 0.33\n",
            "Recall: 0.50\n",
            "F1 Score: 0.40\n",
            "CEBU CITY, Philippines ‚Äì Former Cebu City Mayor Tomas Osme√±a vowed to reveal more information about the extra-judicial killings (EJKs) here in the next House committee hearing. Osme√±a made this statement on Thursday, September 26, after announcing that the hearing scheduled this Friday, September 27, has been postponed to mid-October. However, the former mayor, who was once again invited to shed more light about reports of EJKs here, shared that this development would allow him to present and reveal more information, including records from victims of EJKs.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}