# -*- coding: utf-8 -*-
"""Evil_Geniuses_BERTBasedSummarizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mzsCq5_qMLJRStlBHnv8Q013Ucpto_yE
"""

# Install necessary packages (uncomment in Colab if not already installed)
!pip install transformers
!pip install rouge-score
!pip install sklearn

from transformers import pipeline
from rouge_score import rouge_scorer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Initialize the summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Function to generate summary and calculate metrics
def summarize_text(text):
    if not text.strip():
        return {"error": "Please enter some text to summarize."}

    # Generate summary
    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']

    # Calculate word counts
    input_words = len(text.split())
    summary_words = len(summary.split())

    # Calculate ROUGE scores
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(text, summary)

    # Create binary labels for token presence
    text_tokens = text.lower().split()
    summary_tokens = summary.lower().split()
    y_true = [1 if token in summary_tokens else 0 for token in text_tokens]
    y_pred = [1 if token in text_tokens else 0 for token in summary_tokens]

    # Calculate sklearn metrics
    precision, recall, f1, _ = precision_recall_fscore_support(y_true[:len(y_pred)], y_pred, average='binary')
    accuracy = accuracy_score(y_true[:len(y_pred)], y_pred)

    # Compile results
    return {
        'summary': summary,
        'original_text': text,
        'metrics': {
            'input_words': input_words,
            'summary_words': summary_words,
            'rouge1_f1': round(scores['rouge1'].fmeasure, 4),
            'rouge2_f1': round(scores['rouge2'].fmeasure, 4),
            'rougeL_f1': round(scores['rougeL'].fmeasure, 4),
            'f1_score_sklearn': round(f1, 4),
            'accuracy': round(accuracy, 4)
        }
    }

# User input and summarization
print("Enter the text you want to summarize (press Enter twice when done):")
lines = []
while True:
    line = input()
    if line == "":
        break
    lines.append(line)
user_text = "\n".join(lines)

# Summarize and display results
results = summarize_text(user_text)
if "error" in results:
    print(results["error"])
else:
    print("\nOriginal Text:")
    print("-" * 50)
    print(results['original_text'])
    print("\nGenerated Summary:")
    print("-" * 50)
    print(results['summary'])
    print("\nMetrics:")
    for metric, value in results['metrics'].items():
        print(f"{metric}: {value}")

import matplotlib.pyplot as plt

if "error" not in results:
    metrics = results['metrics']

    # Separate the metrics
    word_metrics = {k: v for k, v in metrics.items() if "words" in k}
    performance_metrics = {k: v for k, v in metrics.items() if "words" not in k}

    # Plotting word counts
    plt.figure(figsize=(8, 4))
    plt.bar(word_metrics.keys(), word_metrics.values(), color='lightgreen')
    plt.ylabel('Word Count')
    plt.title('Input vs. Summary Word Count')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

    # Plotting performance metrics
    plt.figure(figsize=(10, 6))
    plt.barh(list(performance_metrics.keys()), list(performance_metrics.values()), color='skyblue')
    plt.xlabel('Scores')
    plt.xlim(0, 1)  # Ensure the x-axis ranges from 0 to 1
    plt.title('Performance Metrics (ROUGE, F1, Accuracy)')
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    plt.show()